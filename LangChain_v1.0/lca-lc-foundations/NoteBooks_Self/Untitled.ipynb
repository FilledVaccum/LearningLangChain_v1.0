{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5164c733-3738-4d5d-8c2f-6041e2a98051",
   "metadata": {},
   "source": [
    "##  **Key Methods for Objects Initialized with `initChatModel`**\n",
    "\n",
    "An object created using <span style=\"color:#6A1B9A;\"><b>`initChatModel`</b></span> exposes several core methods to interact with a chat-based LLM efficiently.\n",
    "\n",
    "---\n",
    "\n",
    "##  **1. `invoke()` ‚Äî Direct Model Invocation**\n",
    "\n",
    "<span style=\"color:#1565C0;\"><b>The most straightforward way to call a model</b></span>\n",
    "\n",
    "- Used to send **a single message** or **a list of messages**\n",
    "- Commonly used for **synchronous** requests\n",
    "- Supports full **conversation history**\n",
    "\n",
    "###  Key Points\n",
    "- A list of messages represents the **conversation context**\n",
    "- Each message can include a **role** indicating who sent it:\n",
    "  - `system`\n",
    "  - `human`\n",
    "  - `ai`\n",
    "  - `tool`\n",
    "\n",
    "> ‚úÖ Best for **simple requests**, testing, and deterministic flows\n",
    "\n",
    "---\n",
    "\n",
    "##  **2. `stream()` ‚Äî Real-Time Output Streaming**\n",
    "\n",
    "<span style=\"color:#2E7D32;\"><b>Streams model output as it is generated</b></span>\n",
    "\n",
    "- Ideal for **long responses**\n",
    "- Improves perceived latency and **user experience**\n",
    "- Returns an **iterator** yielding output chunks incrementally\n",
    "\n",
    "###  How It Works\n",
    "- Call `stream()`\n",
    "- Iterate over chunks in a loop\n",
    "- Process or display output **in real time**\n",
    "\n",
    ">  Best for **interactive UIs**, chat applications, and agent reasoning traces\n",
    "\n",
    "---\n",
    "\n",
    "##  **3. `batch()` ‚Äî Parallel Request Processing**\n",
    "\n",
    "<span style=\"color:#EF6C00;\"><b>Executes multiple independent requests together</b></span>\n",
    "\n",
    "- Processes inputs **in parallel**\n",
    "- Improves **throughput**\n",
    "- Reduces **cost per request**\n",
    "\n",
    "###  Benefits\n",
    "- Efficient for:\n",
    "  - Evaluations\n",
    "  - Bulk inference\n",
    "  - Background processing\n",
    "- Minimizes repeated model overhead\n",
    "\n",
    ">  Best for **high-volume workloads** and **cost-optimized pipelines**\n",
    "\n",
    "---\n",
    "\n",
    "##  **Summary Table**\n",
    "\n",
    "| Method    | Use Case                          | Execution Style | Best For |\n",
    "|----------|-----------------------------------|-----------------|----------|\n",
    "| `invoke` | Single / conversational request   | Synchronous     | Simplicity |\n",
    "| `stream` | Progressive response generation   | Iterative       | UX & interactivity |\n",
    "| `batch`  | Multiple independent requests     | Parallel        | Scale & cost |\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ **Key Takeaway**\n",
    "> <span style=\"color:#C62828;\"><b>Choose the method based on interaction style:</b></span>  \n",
    "> **`invoke` ‚Üí simplicity**, **`stream` ‚Üí experience**, **`batch` ‚Üí scale**\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c05e125-6ac5-4cb1-b3a7-fcd8b578e582",
   "metadata": {},
   "source": [
    "##  **`messages` ‚Äî Core Input for Model Invocation**\n",
    "\n",
    "<span style=\"color:#6A1B9A;\"><b>`messages`</b></span> is the **primary input key** used when invoking a model in **LangChain**.  \n",
    "It defines the **entire conversational context** passed to the LLM.\n",
    "\n",
    "---\n",
    "\n",
    "##  **What Are Messages?**\n",
    "\n",
    "**Messages** are the **fundamental unit of context** for models in LangChain.  \n",
    "They represent both **inputs and outputs**, carrying:\n",
    "\n",
    "- **Content**\n",
    "- **Role**\n",
    "- **Metadata**\n",
    "\n",
    "Together, these define the **state of the conversation** when interacting with an LLM.\n",
    "\n",
    "---\n",
    "\n",
    "## **Message Structure**\n",
    "\n",
    "Each message typically contains the following components:\n",
    "\n",
    "### **Role**\n",
    "<span style=\"color:#1565C0;\"><b>Identifies the message type</b></span>  \n",
    "Examples:\n",
    "- `system`\n",
    "- `user`\n",
    "- `assistant`\n",
    "- `tool`\n",
    "\n",
    "---\n",
    "\n",
    "### **Content**\n",
    "<span style=\"color:#2E7D32;\"><b>The actual payload of the message</b></span>  \n",
    "May include:\n",
    "- Text\n",
    "- Images\n",
    "- Audio\n",
    "- Documents\n",
    "- Tool instructions\n",
    "\n",
    "---\n",
    "\n",
    "### **Metadata** *(Optional)*\n",
    "<span style=\"color:#EF6C00;\"><b>Additional contextual information</b></span> such as:\n",
    "- Message IDs\n",
    "- Token usage\n",
    "- Response timing\n",
    "- Tool execution details\n",
    "\n",
    "---\n",
    "\n",
    "## **Message Types in LangChain**\n",
    "\n",
    "### **System Message**\n",
    "<span style=\"color:#5E35B1;\"><i>Defines behavior and global context</i></span>  \n",
    "- Sets rules, tone, and constraints\n",
    "- Applied implicitly across the conversation\n",
    "\n",
    "---\n",
    "\n",
    "### **Human Message**\n",
    "<span style=\"color:#1B5E20;\"><i>Represents user input</i></span>  \n",
    "- Questions\n",
    "- Instructions\n",
    "- Prompts\n",
    "\n",
    "---\n",
    "\n",
    "### **AI Message**\n",
    "<span style=\"color:#0D47A1;\"><i>Generated by the model</i></span>  \n",
    "Includes:\n",
    "- Text responses\n",
    "- Tool call requests\n",
    "- Model-generated metadata\n",
    "\n",
    "---\n",
    "\n",
    "### **Tool Message**\n",
    "<span style=\"color:#E65100;\"><i>Output from tool execution</i></span>  \n",
    "- Results returned by tools\n",
    "- Fed back into the model for continued reasoning\n",
    "\n",
    "---\n",
    "\n",
    "## **Key Insight**\n",
    "> <span style=\"color:#C62828;\"><b>`messages` are not just text</b></span> ‚Äî they are a **structured conversation state** that enables:\n",
    "- Multi-turn reasoning\n",
    "- Tool usage\n",
    "- Memory and context persistence\n",
    "- Agentic workflows\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620a36f5-5444-4a03-9e08-a11d18f7430e",
   "metadata": {},
   "source": [
    "# `init_chat_model` parameters (LangChain)\n",
    "\n",
    "`init_chat_model` creates a provider-agnostic **chat** model from a name and optional configuration.\n",
    "\n",
    "---\n",
    "\n",
    "## Core signature\n",
    "\n",
    "```python\n",
    "from langchain.chat_models import init_chat_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac0cea1-cf8d-4514-95e9-c81c24fd3206",
   "metadata": {},
   "source": [
    "```python\n",
    "init_chat_model(\n",
    "    model: str | None = None,\n",
    "    *,\n",
    "    model_provider: str | None = None,\n",
    "    configurable_fields: Literal[\"any\"] | list[str] | tuple[str, ...] | None = None,\n",
    "    config_prefix: str | None = None,\n",
    "    **kwargs: Any,\n",
    ") -> BaseChatModel | _ConfigurableModel\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d1df6f-511c-46a8-8926-97a7f8af565a",
   "metadata": {},
   "source": [
    "## **Invoking a Chat Model Using `init_chat_model`**\n",
    "\n",
    "This example demonstrates how to initialize and invoke a chat model using LangChain with minimal configuration.\n",
    "\n",
    "---\n",
    "\n",
    "## **Python Example**\n",
    "\n",
    "```python\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "model = init_chat_model(\"openai:gpt-5-nano\")\n",
    "resp = model.invoke(\"Hello, who are you?\")\n",
    "print(resp.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9edd64e-df9e-477b-825f-872689ef2e0c",
   "metadata": {},
   "source": [
    "## **Explicit Chat Model Configuration with `init_chat_model`**\n",
    "\n",
    "This example shows how to **explicitly configure** the model, provider, endpoint, and credentials instead of relying on inference from the model string or environment variables.\n",
    "\n",
    "---\n",
    "\n",
    "## **Python Example**\n",
    "\n",
    "```python\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "model = init_chat_model(\n",
    "    model=\"MODEL_NAME\",\n",
    "    model_provider=\"openai\",\n",
    "    base_url=\"https://your-endpoint.example.com/v1\",\n",
    "    api_key=\"YOUR_API_KEY\",\n",
    ")\n",
    "\n",
    "resp = model.invoke(\"Say hi in one short sentence.\")\n",
    "print(resp.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d96511-d44d-434e-926e-788b5757b547",
   "metadata": {},
   "source": [
    "## **Main Parameters in the `init_chat_model` Configuration Pattern**\n",
    "\n",
    "This pattern allows fine-grained control over **model selection**, **provider routing**, and **runtime behavior** when initializing a chat model.\n",
    "\n",
    "---\n",
    "\n",
    "## **Core Parameters**\n",
    "\n",
    "### **`model`**\n",
    "<span style=\"color:#6A1B9A;\"><b>Logical model identifier</b></span>\n",
    "\n",
    "- Represents the target model name\n",
    "- Examples:\n",
    "  - `gpt-4.1`\n",
    "  - `MODEL_NAME`\n",
    "- Can map to:\n",
    "  - A hosted provider model\n",
    "  - A custom or internally aliased model\n",
    "\n",
    "---\n",
    "\n",
    "### **`model_provider`**\n",
    "<span style=\"color:#1565C0;\"><b>Provider key used for client resolution</b></span>\n",
    "\n",
    "- Determines which provider implementation is used\n",
    "- Common values:\n",
    "  - `openai`\n",
    "  - `anthropic`\n",
    "  - `vertexai`\n",
    "- Required when the provider cannot be inferred automatically\n",
    "\n",
    "---\n",
    "\n",
    "### **`base_url`**\n",
    "<span style=\"color:#2E7D32;\"><b>Optional custom HTTP endpoint</b></span>\n",
    "\n",
    "- Overrides the default provider endpoint\n",
    "- Common use cases:\n",
    "  - Enterprise proxies\n",
    "  - Self-hosted or OpenAI-compatible APIs\n",
    "  - Traffic routing through gateways\n",
    "\n",
    "---\n",
    "\n",
    "### **`api_key`**\n",
    "<span style=\"color:#EF6C00;\"><b>Optional authentication credential</b></span>\n",
    "\n",
    "- Used when environment variables are not preferred\n",
    "- Enables:\n",
    "  - Per-request credential injection\n",
    "  - Multi-tenant or dynamic authentication\n",
    "\n",
    "> In production environments, secret managers or environment variables are recommended.\n",
    "\n",
    "---\n",
    "\n",
    "## **Provider-Specific Arguments (`**kwargs`)**\n",
    "\n",
    "<span style=\"color:#455A64;\"><b>Additional parameters forwarded to the provider client</b></span>\n",
    "\n",
    "- Passed directly to the underlying SDK\n",
    "- Behavior depends on provider support\n",
    "\n",
    "### **Common Examples**\n",
    "- `temperature`\n",
    "- `max_tokens`\n",
    "- `top_p`\n",
    "- `frequency_penalty`\n",
    "\n",
    "These parameters control **generation behavior**, **output length**, and **sampling strategy**.\n",
    "\n",
    "---\n",
    "\n",
    "## **Key Insight**\n",
    "\n",
    "> <span style=\"color:#C62828;\"><b>This pattern cleanly separates concerns</b></span>:\n",
    "- Logical model naming\n",
    "- Provider selection\n",
    "- Endpoint routing\n",
    "- Runtime tuning via `**kwargs`\n",
    "\n",
    "This makes the configuration **portable**, **extensible**, and **enterprise-ready**.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe6a01a-2a9b-45ea-a785-d36bb8df5104",
   "metadata": {},
   "source": [
    "## **Passing Runtime Parameters via `**kwargs` in `init_chat_model`**\n",
    "\n",
    "This example demonstrates how **generation parameters** are passed directly to the underlying provider client using `**kwargs`.\n",
    "\n",
    "---\n",
    "\n",
    "## **Python Example**\n",
    "\n",
    "```python\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "model = init_chat_model(\n",
    "    model=\"openai:gpt-4.1-mini\",\n",
    "    temperature=0.2,\n",
    "    max_tokens=256,\n",
    ")\n",
    "\n",
    "resp = model.invoke(\"Explain RAG in 3 bullet points.\")\n",
    "print(resp.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c11cd21-5a28-4d6d-9e52-35f6d32efe60",
   "metadata": {},
   "source": [
    "## **Runtime Configuration Overrides with `configurable_fields`**\n",
    "\n",
    "This pattern demonstrates how to define **which parameters can be overridden at runtime** when initializing a chat model, without rebuilding the model object.\n",
    "\n",
    "---\n",
    "\n",
    "## **Python Example**\n",
    "\n",
    "```python\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "chat = init_chat_model(\n",
    "    model=\"openai:gpt-4.1-mini\",\n",
    "    temperature=0,\n",
    "    configurable_fields=(\"model\", \"temperature\", \"max_tokens\"),\n",
    "    config_prefix=\"chat\",\n",
    ")\n",
    "\n",
    "# Uses defaults: model=\"openai:gpt-4.1-mini\", temperature=0\n",
    "resp_default = chat.invoke(\"What's your name?\")\n",
    "print(resp_default.content)\n",
    "\n",
    "# Override via config at runtime\n",
    "resp_custom = chat.invoke(\n",
    "    \"What's your name?\",\n",
    "    config={\n",
    "        \"configurable\": {\n",
    "            \"chat_model\": \"openai:gpt-4.1-mini\",\n",
    "            \"chat_temperature\": 0.7,\n",
    "            \"chat_max_tokens\": 128,\n",
    "        }\n",
    "    },\n",
    ")\n",
    "print(resp_custom.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831e30db-1804-4ba9-b97a-aa9c39fefcc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1404b4-99ec-4429-a068-9dc657e82433",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b9bee9-5c93-47ca-abff-8aa962e74f73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f3eefc0-9d57-4457-aa1f-fcb532a6ffd3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T06:44:40.950539Z",
     "iopub.status.busy": "2026-01-10T06:44:40.950286Z",
     "iopub.status.idle": "2026-01-10T06:44:40.960519Z",
     "shell.execute_reply": "2026-01-10T06:44:40.959692Z",
     "shell.execute_reply.started": "2026-01-10T06:44:40.950521Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1b0aefc-23fd-442c-ac87-abd54179df78",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T06:45:01.450284Z",
     "iopub.status.busy": "2026-01-10T06:45:01.450066Z",
     "iopub.status.idle": "2026-01-10T06:45:02.493329Z",
     "shell.execute_reply": "2026-01-10T06:45:02.492318Z",
     "shell.execute_reply.started": "2026-01-10T06:45:01.450267Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "model = init_chat_model(model=\"gpt-5-nano\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9798a45d-d4e3-4971-ae58-b150b9b84e4e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T06:45:02.494181Z",
     "iopub.status.busy": "2026-01-10T06:45:02.493970Z",
     "iopub.status.idle": "2026-01-10T06:45:23.066231Z",
     "shell.execute_reply": "2026-01-10T06:45:23.065134Z",
     "shell.execute_reply.started": "2026-01-10T06:45:02.494165Z"
    }
   },
   "outputs": [],
   "source": [
    "response = model.invoke(\"What's the problem of Delhi in Winters?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "725dcff7-b0ce-4754-9077-d6cc4a4f8cf8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T06:45:23.067034Z",
     "iopub.status.busy": "2026-01-10T06:45:23.066875Z",
     "iopub.status.idle": "2026-01-10T06:45:23.071474Z",
     "shell.execute_reply": "2026-01-10T06:45:23.070484Z",
     "shell.execute_reply.started": "2026-01-10T06:45:23.067019Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Short answer: Delhi‚Äôs winter pollution is a big problem because a lot of pollution gets trapped by weather and geography, while emissions from vehicles, industry, construction, and crop burning stay high. The result is very high levels of PM2.5/PM10 that can blanket the city for weeks.\\n\\nWhat causes it\\n- Weather and atmosphere:\\n  - In winter, the air near the ground cools and forms a temperature inversion, which traps pollutants close to the surface.\\n  - Winds are often light, and the mixing height (how far pollutants can disperse upward) is low, so pollutants don‚Äôt get carried away.\\n  - Fog and humidity can make the haze feel thicker and reduce visibility.\\n- Local pollution sources:\\n  - Vehicles (cars, trucks, buses), diesel generators, industry, construction dust, and burning of waste.\\n- Regional contribution:\\n  - Burning of crop residue (stubble burning) in neighboring states (Punjab, Haryana, Uttar Pradesh) sends large plumes that travel into Delhi, piling onto local pollution.\\n- Geography:\\n  - Delhi sits in the Indo-Gangetic Plain, with surrounding air and limited vertical mixing in winter, which makes pollution accumulate more easily.\\n\\nHealth and daily life impact\\n- Particulate matter (PM2.5 and PM10) levels often become very high, leading to respiratory and heart problems, worsened asthma, eye irritation, and increased hospital visits.\\n- Outdoor activities can be restricted or risky on days with very high AQI (Air Quality Index).\\n\\nWhat‚Äôs being done\\n- Authorities use a graded response system (GRAP) to impose escalating restrictions on polluting activities during bad air days.\\n- There are efforts to curb emissions (older vehicles, industrial emissions, construction dust, waste burning) and to promote cleaner energy and mobility, though results vary by year and season.\\n- Individuals can help by monitoring air quality and taking precautions on high-pollution days.\\n\\nWhat you can do (if you‚Äôre in Delhi or visiting)\\n- Check the daily AQI and stay indoors or limit outdoor activity on bad days.\\n- Use air purifiers at home and, if outdoors, wear a well-fitted N95/KN95 mask.\\n- Keep windows closed during peak pollution times; use exhaust fans with proper filtration if indoors.\\n- Avoid burning trash or other materials; minimize outdoor burning if possible.\\n- Use public transport, carpool, or cleaner mobility options to reduce local emissions.\\n\\nWould you like more detail on one of these parts‚Äîhealth effects, how GRAP works, or what Delhi is doing right now to improve air quality?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 2198, 'prompt_tokens': 14, 'total_tokens': 2212, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 1664, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-5-nano-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-CwN0ZqYDyTx1YKYKV120Zzb4y0nHf', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019ba6a6-b7a1-73e1-aa9d-9d0ece7e5c3a-0', usage_metadata={'input_tokens': 14, 'output_tokens': 2198, 'total_tokens': 2212, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 1664}})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a7dc079-e3ce-40d6-9d3f-01fa9157ccfa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T06:45:23.072245Z",
     "iopub.status.busy": "2026-01-10T06:45:23.072044Z",
     "iopub.status.idle": "2026-01-10T06:45:23.075854Z",
     "shell.execute_reply": "2026-01-10T06:45:23.075023Z",
     "shell.execute_reply.started": "2026-01-10T06:45:23.072230Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Short answer: Delhi‚Äôs winter pollution is a big problem because a lot of pollution gets trapped by weather and geography, while emissions from vehicles, industry, construction, and crop burning stay high. The result is very high levels of PM2.5/PM10 that can blanket the city for weeks.\n",
      "\n",
      "What causes it\n",
      "- Weather and atmosphere:\n",
      "  - In winter, the air near the ground cools and forms a temperature inversion, which traps pollutants close to the surface.\n",
      "  - Winds are often light, and the mixing height (how far pollutants can disperse upward) is low, so pollutants don‚Äôt get carried away.\n",
      "  - Fog and humidity can make the haze feel thicker and reduce visibility.\n",
      "- Local pollution sources:\n",
      "  - Vehicles (cars, trucks, buses), diesel generators, industry, construction dust, and burning of waste.\n",
      "- Regional contribution:\n",
      "  - Burning of crop residue (stubble burning) in neighboring states (Punjab, Haryana, Uttar Pradesh) sends large plumes that travel into Delhi, piling onto local pollution.\n",
      "- Geography:\n",
      "  - Delhi sits in the Indo-Gangetic Plain, with surrounding air and limited vertical mixing in winter, which makes pollution accumulate more easily.\n",
      "\n",
      "Health and daily life impact\n",
      "- Particulate matter (PM2.5 and PM10) levels often become very high, leading to respiratory and heart problems, worsened asthma, eye irritation, and increased hospital visits.\n",
      "- Outdoor activities can be restricted or risky on days with very high AQI (Air Quality Index).\n",
      "\n",
      "What‚Äôs being done\n",
      "- Authorities use a graded response system (GRAP) to impose escalating restrictions on polluting activities during bad air days.\n",
      "- There are efforts to curb emissions (older vehicles, industrial emissions, construction dust, waste burning) and to promote cleaner energy and mobility, though results vary by year and season.\n",
      "- Individuals can help by monitoring air quality and taking precautions on high-pollution days.\n",
      "\n",
      "What you can do (if you‚Äôre in Delhi or visiting)\n",
      "- Check the daily AQI and stay indoors or limit outdoor activity on bad days.\n",
      "- Use air purifiers at home and, if outdoors, wear a well-fitted N95/KN95 mask.\n",
      "- Keep windows closed during peak pollution times; use exhaust fans with proper filtration if indoors.\n",
      "- Avoid burning trash or other materials; minimize outdoor burning if possible.\n",
      "- Use public transport, carpool, or cleaner mobility options to reduce local emissions.\n",
      "\n",
      "Would you like more detail on one of these parts‚Äîhealth effects, how GRAP works, or what Delhi is doing right now to improve air quality?\n"
     ]
    }
   ],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368481d2-0cce-4bc3-8be5-0f5baaf3ab66",
   "metadata": {},
   "source": [
    "```Type: AIMessage object (not a dict or list)\n",
    "\n",
    "Key attributes:\n",
    "- content (str): The main response text\n",
    "- additional_kwargs (dict): Extra metadata like refusal\n",
    "- response_metadata (dict): Contains:\n",
    "  - token_usage: Token counts and details\n",
    "  - model_provider: \"openai\" \n",
    "  - model_name: \"gpt-5-nano-2025-08-07\"\n",
    "  - system_fingerprint, id, service_tier, finish_reason, logprobs\n",
    "- id (str): Unique run identifier\n",
    "- tool_calls (list): Empty list\n",
    "- invalid_tool_calls (list): Empty list  \n",
    "- usage_metadata (dict): Token usage summary\n",
    "\n",
    "Access examples:\n",
    "python\n",
    "x.content  # Get the text response\n",
    "x.response_metadata['token_usage']['total_tokens']  # Get token count\n",
    "x.response_metadata['model_name']  # Get model used\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5d2d406-538d-487a-b1f3-c0377d3cc12e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T06:45:23.076968Z",
     "iopub.status.busy": "2026-01-10T06:45:23.076749Z",
     "iopub.status.idle": "2026-01-10T06:45:23.081601Z",
     "shell.execute_reply": "2026-01-10T06:45:23.080570Z",
     "shell.execute_reply.started": "2026-01-10T06:45:23.076946Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'finish_reason': 'stop',\n",
      " 'id': 'chatcmpl-CwN0ZqYDyTx1YKYKV120Zzb4y0nHf',\n",
      " 'logprobs': None,\n",
      " 'model_name': 'gpt-5-nano-2025-08-07',\n",
      " 'model_provider': 'openai',\n",
      " 'service_tier': 'default',\n",
      " 'system_fingerprint': None,\n",
      " 'token_usage': {'completion_tokens': 2198,\n",
      "                 'completion_tokens_details': {'accepted_prediction_tokens': 0,\n",
      "                                               'audio_tokens': 0,\n",
      "                                               'reasoning_tokens': 1664,\n",
      "                                               'rejected_prediction_tokens': 0},\n",
      "                 'prompt_tokens': 14,\n",
      "                 'prompt_tokens_details': {'audio_tokens': 0,\n",
      "                                           'cached_tokens': 0},\n",
      "                 'total_tokens': 2212}}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(response.response_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa54c203-9a13-4069-b60f-6cfd2f8ef9df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T06:45:23.082721Z",
     "iopub.status.busy": "2026-01-10T06:45:23.082498Z",
     "iopub.status.idle": "2026-01-10T06:45:23.086804Z",
     "shell.execute_reply": "2026-01-10T06:45:23.085697Z",
     "shell.execute_reply.started": "2026-01-10T06:45:23.082700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'completion_tokens': 2198,\n",
      " 'completion_tokens_details': {'accepted_prediction_tokens': 0,\n",
      "                               'audio_tokens': 0,\n",
      "                               'reasoning_tokens': 1664,\n",
      "                               'rejected_prediction_tokens': 0},\n",
      " 'prompt_tokens': 14,\n",
      " 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0},\n",
      " 'total_tokens': 2212}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(response.response_metadata['token_usage'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8350d64-0ffa-4fbd-b843-2a2af9520fbe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T06:45:23.087645Z",
     "iopub.status.busy": "2026-01-10T06:45:23.087429Z",
     "iopub.status.idle": "2026-01-10T06:45:23.092404Z",
     "shell.execute_reply": "2026-01-10T06:45:23.091394Z",
     "shell.execute_reply.started": "2026-01-10T06:45:23.087629Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2212\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(response.response_metadata['token_usage']['total_tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42810aea-198a-41a7-a498-ee4d8e4841ba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T06:45:23.093057Z",
     "iopub.status.busy": "2026-01-10T06:45:23.092905Z",
     "iopub.status.idle": "2026-01-10T06:45:23.098357Z",
     "shell.execute_reply": "2026-01-10T06:45:23.097427Z",
     "shell.execute_reply.started": "2026-01-10T06:45:23.093043Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__abstractmethods__',\n",
       " '__add__',\n",
       " '__annotations__',\n",
       " '__class__',\n",
       " '__class_getitem__',\n",
       " '__class_vars__',\n",
       " '__copy__',\n",
       " '__deepcopy__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__fields__',\n",
       " '__fields_set__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__get_pydantic_core_schema__',\n",
       " '__get_pydantic_json_schema__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__iter__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__pretty__',\n",
       " '__private_attributes__',\n",
       " '__pydantic_complete__',\n",
       " '__pydantic_computed_fields__',\n",
       " '__pydantic_core_schema__',\n",
       " '__pydantic_custom_init__',\n",
       " '__pydantic_decorators__',\n",
       " '__pydantic_extra__',\n",
       " '__pydantic_fields__',\n",
       " '__pydantic_fields_set__',\n",
       " '__pydantic_generic_metadata__',\n",
       " '__pydantic_init_subclass__',\n",
       " '__pydantic_on_complete__',\n",
       " '__pydantic_parent_namespace__',\n",
       " '__pydantic_post_init__',\n",
       " '__pydantic_private__',\n",
       " '__pydantic_root_model__',\n",
       " '__pydantic_serializer__',\n",
       " '__pydantic_setattr_handlers__',\n",
       " '__pydantic_validator__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__replace__',\n",
       " '__repr__',\n",
       " '__repr_args__',\n",
       " '__repr_name__',\n",
       " '__repr_recursion__',\n",
       " '__repr_str__',\n",
       " '__rich_repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__signature__',\n",
       " '__sizeof__',\n",
       " '__slots__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_abc_impl',\n",
       " '_backwards_compat_tool_calls',\n",
       " '_calculate_keys',\n",
       " '_copy_and_set_values',\n",
       " '_get_value',\n",
       " '_iter',\n",
       " '_setattr_handler',\n",
       " 'additional_kwargs',\n",
       " 'construct',\n",
       " 'content',\n",
       " 'content_blocks',\n",
       " 'copy',\n",
       " 'dict',\n",
       " 'from_orm',\n",
       " 'get_lc_namespace',\n",
       " 'id',\n",
       " 'invalid_tool_calls',\n",
       " 'is_lc_serializable',\n",
       " 'json',\n",
       " 'lc_attributes',\n",
       " 'lc_id',\n",
       " 'lc_secrets',\n",
       " 'model_computed_fields',\n",
       " 'model_config',\n",
       " 'model_construct',\n",
       " 'model_copy',\n",
       " 'model_dump',\n",
       " 'model_dump_json',\n",
       " 'model_extra',\n",
       " 'model_fields',\n",
       " 'model_fields_set',\n",
       " 'model_json_schema',\n",
       " 'model_parametrized_name',\n",
       " 'model_post_init',\n",
       " 'model_rebuild',\n",
       " 'model_validate',\n",
       " 'model_validate_json',\n",
       " 'model_validate_strings',\n",
       " 'name',\n",
       " 'parse_file',\n",
       " 'parse_obj',\n",
       " 'parse_raw',\n",
       " 'pretty_print',\n",
       " 'pretty_repr',\n",
       " 'response_metadata',\n",
       " 'schema',\n",
       " 'schema_json',\n",
       " 'text',\n",
       " 'to_json',\n",
       " 'to_json_not_implemented',\n",
       " 'tool_calls',\n",
       " 'type',\n",
       " 'update_forward_refs',\n",
       " 'usage_metadata',\n",
       " 'validate']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Shows all attributes/methods\n",
    "dir(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d55387e-2f72-48b7-ae46-829c9c1b13f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T06:45:23.099225Z",
     "iopub.status.busy": "2026-01-10T06:45:23.099015Z",
     "iopub.status.idle": "2026-01-10T06:45:23.103750Z",
     "shell.execute_reply": "2026-01-10T06:45:23.102659Z",
     "shell.execute_reply.started": "2026-01-10T06:45:23.099200Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['token_usage', 'model_provider', 'model_name', 'system_fingerprint', 'id', 'service_tier', 'finish_reason', 'logprobs'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show Dictonary Keys\n",
    "response.response_metadata.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32db93f-d3e1-4223-b69c-700f7f50daaa",
   "metadata": {},
   "source": [
    "## Custominzing the model using prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "90d8d1d9-678c-437e-938d-e4d89dd68497",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T06:45:23.104547Z",
     "iopub.status.busy": "2026-01-10T06:45:23.104325Z",
     "iopub.status.idle": "2026-01-10T06:45:31.657403Z",
     "shell.execute_reply": "2026-01-10T06:45:31.656382Z",
     "shell.execute_reply.started": "2026-01-10T06:45:23.104525Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('There isn‚Äôt one. The Milky Way isn‚Äôt a country with a capital. It‚Äôs a '\n",
      " 'galaxy, and its central region‚Äîoften called the Galactic Center‚Äîhosts a '\n",
      " 'supermassive black hole named Sagittarius A* (Sgr A*). It‚Äôs about '\n",
      " '26,000‚Äì28,000 light-years from Earth, in the direction of the constellation '\n",
      " 'Sagittarius, with a mass of roughly 4 million solar masses. If you meant a '\n",
      " 'fictional ‚Äúcapital,‚Äù tell me the setting and I can help brainstorm.')\n"
     ]
    }
   ],
   "source": [
    "model = init_chat_model(\n",
    "    model = 'gpt-5-nano',\n",
    "    temperature=1.0\n",
    ")\n",
    "\n",
    "response = model.invoke(\"What's the Capital of Milky Way?\")\n",
    "pprint(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "43505802-54df-4777-992a-f3662de3ec4f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T06:45:31.658172Z",
     "iopub.status.busy": "2026-01-10T06:45:31.658009Z",
     "iopub.status.idle": "2026-01-10T06:45:31.661923Z",
     "shell.execute_reply": "2026-01-10T06:45:31.660930Z",
     "shell.execute_reply.started": "2026-01-10T06:45:31.658157Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sagemaker-user/LangChain/LangChain_v1.0/lca-lc-foundations/.venv/bin/python3\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a151a40d-04e3-4d65-9533-794c78852540",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T06:45:31.662564Z",
     "iopub.status.busy": "2026-01-10T06:45:31.662405Z",
     "iopub.status.idle": "2026-01-10T06:45:37.907206Z",
     "shell.execute_reply": "2026-01-10T06:45:37.906263Z",
     "shell.execute_reply.started": "2026-01-10T06:45:31.662549Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absolutely! Let's get that YoYo Honey Singh vibe going. Here are a few ways to complete that line, depending on what kind of flavor you're going for:\n",
      "\n",
      "**Focusing on the \"Aloo Kai Pakodai\" imagery (delicious, common, relatable):**\n",
      "\n",
      "*   \"...jaisai aaloo kai pakodai, **garam garam, ekdum tasty**.\" (Hot hot, totally tasty)\n",
      "*   \"...jaisai aaloo kai pakodai, **sabko bhaye, kya ladki, kya ladka**.\" (Loved by everyone, what girl, what boy)\n",
      "*   \"...jaisai aaloo kai pakodai, **chatpate, nakhre wale, sabko pakde**.\" (Tangy, fussy/playful, catch everyone)\n",
      "*   \"...jaisai aaloo kai pakodai, **dil ko lubhaye, jabardast, lagaye**.\" (Entices the heart, awesome, makes you want more)\n",
      "\n",
      "**Adding a bit more Honey Singh swagger and relatable scenarios:**\n",
      "\n",
      "*   \"...jaisai aaloo kai pakodai, **party mein sabki favourite, ho jaaye shor machaai**.\" (Everyone's favourite at the party, let's make some noise)\n",
      "*   \"...jaisai aaloo kai pakodai, **college ke din, crush mile, ho jaaye mushkil, dil ko samjhaai**.\" (College days, meet a crush, it gets difficult, to make the heart understand)\n",
      "*   \"...jaisai aaloo kai pakodai, **choti choti baatein, bade bade sapne, sabko mil jaaye, wohi hai apna style**.\" (Small small talks, big big dreams, everyone gets them, that's our style)\n",
      "\n",
      "**A bit more playful and direct:**\n",
      "\n",
      "*   \"...jaisai aaloo kai pakodai, **kabhi na thake, kabhi na ruke, full on masti, no fikr, no bhagya**.\" (Never tire, never stop, full on fun, no worries, no running away)\n",
      "*   \"...jaisai aaloo kai pakodai, **aankhon mein chamak, honthon pe hassi, sabko karein crazy, yahi hai asli masti**.\" (Sparkle in the eyes, smile on the lips, make everyone crazy, this is the real fun)\n",
      "\n",
      "**To give you the BEST completion, tell me:**\n",
      "\n",
      "*   **What kind of vibe are you going for?** (Happy, romantic, boastful, energetic?)\n",
      "*   **What's the general theme of the song you're imagining?**\n",
      "\n",
      "Once you give me a little more direction, I can tailor the YoYo Honey Singh magic perfectly! üé§‚ú®\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "model = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash-lite\")\n",
    "\n",
    "response = model.invoke(\"Complete this YoYo Honey Singh, jaisai aaloo kai pakodai\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c342ec48-21f8-4b01-b7cc-4cd244bd59c1",
   "metadata": {},
   "source": [
    "## Initialising and invoking an agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0ab4ab58-9407-4eed-b0e4-1201e9499129",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T06:45:37.908332Z",
     "iopub.status.busy": "2026-01-10T06:45:37.908012Z",
     "iopub.status.idle": "2026-01-10T06:45:38.117353Z",
     "shell.execute_reply": "2026-01-10T06:45:38.116200Z",
     "shell.execute_reply.started": "2026-01-10T06:45:37.908315Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "agent = create_agent(model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ff54dfda-857d-48f8-a8f0-8ada33cdb11b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T06:50:30.552975Z",
     "iopub.status.busy": "2026-01-10T06:50:30.552755Z",
     "iopub.status.idle": "2026-01-10T06:50:32.826090Z",
     "shell.execute_reply": "2026-01-10T06:50:32.825037Z",
     "shell.execute_reply.started": "2026-01-10T06:50:30.552960Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [HumanMessage(content='What is Capital of Universe?', additional_kwargs={}, response_metadata={}, id='0da8c77a-f35b-4744-bc8e-c414613512fe'),\n",
      "              AIMessage(content='That\\'s a fascinating question! However, there isn\\'t a \"Capital of the Universe\" in the way we understand capitals of countries or states on Earth.\\n\\nHere\\'s why:\\n\\n*   **No Central Authority:** The universe, as far as we understand it, doesn\\'t have a governing body, a single ruler, or a designated center of power.\\n*   **Vastness and Scale:** The sheer scale of the universe is incomprehensible. It\\'s constantly expanding, and we don\\'t even know its full extent. The concept of a \"capital\" implies a central point that would be recognizable to everyone and everything, which doesn\\'t fit the reality of the cosmos.\\n*   **No Known Inhabitants:** While we search for extraterrestrial life, we haven\\'t discovered any civilization that could establish or claim a \"capital\" for the entire universe.\\n\\n**However, it\\'s a great question for philosophical or imaginative thought!**\\n\\nIf you were to imagine a \"Capital of the Universe,\" what would it be like? What would it represent? Some might think of:\\n\\n*   **A Theoretical Center:** Perhaps a point from which the Big Bang originated, though this is more of a theoretical origin point than a capital.\\n*   **A Place of Great Significance:** Maybe a supermassive black hole, a galaxy with incredible diversity, or a region of space where fundamental laws of physics are most evident.\\n*   **A Spiritual or Philosophical Concept:** Some might see the \"capital\" as an abstract idea, like consciousness, knowledge, or the interconnectedness of all things.\\n\\nSo, while there\\'s no literal \"Capital of the Universe,\" the question opens up a wonderful space for creativity and thinking about our place in the cosmos!', additional_kwargs={}, response_metadata={'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash-lite', 'safety_ratings': [], 'model_provider': 'google_genai'}, id='lc_run--019ba6ab-b9eb-7f72-9aa6-159a76be445b-0', usage_metadata={'input_tokens': 7, 'output_tokens': 364, 'total_tokens': 371, 'input_token_details': {'cache_read': 0}})]}\n"
     ]
    }
   ],
   "source": [
    "from langchain.messages import HumanMessage\n",
    "response = agent.invoke(\n",
    "    { \"messages\": [HumanMessage (content = \"What is Capital of Universe?\") ] }\n",
    ")\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bc156c-de2d-48d3-8a77-87004e65c6b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lca-foundations",
   "language": "python",
   "name": "lca-foundations"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
