{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5164c733-3738-4d5d-8c2f-6041e2a98051",
   "metadata": {},
   "source": [
    "##  **Key Methods for Objects Initialized with `initChatModel`**\n",
    "\n",
    "An object created using <span style=\"color:#6A1B9A;\"><b>`initChatModel`</b></span> exposes several core methods to interact with a chat-based LLM efficiently.\n",
    "\n",
    "---\n",
    "\n",
    "##  **1. `invoke()` — Direct Model Invocation**\n",
    "\n",
    "<span style=\"color:#1565C0;\"><b>The most straightforward way to call a model</b></span>\n",
    "\n",
    "- Used to send **a single message** or **a list of messages**\n",
    "- Commonly used for **synchronous** requests\n",
    "- Supports full **conversation history**\n",
    "\n",
    "###  Key Points\n",
    "- A list of messages represents the **conversation context**\n",
    "- Each message can include a **role** indicating who sent it:\n",
    "  - `system`\n",
    "  - `human`\n",
    "  - `ai`\n",
    "  - `tool`\n",
    "\n",
    "> ✅ Best for **simple requests**, testing, and deterministic flows\n",
    "\n",
    "---\n",
    "\n",
    "##  **2. `stream()` — Real-Time Output Streaming**\n",
    "\n",
    "<span style=\"color:#2E7D32;\"><b>Streams model output as it is generated</b></span>\n",
    "\n",
    "- Ideal for **long responses**\n",
    "- Improves perceived latency and **user experience**\n",
    "- Returns an **iterator** yielding output chunks incrementally\n",
    "\n",
    "###  How It Works\n",
    "- Call `stream()`\n",
    "- Iterate over chunks in a loop\n",
    "- Process or display output **in real time**\n",
    "\n",
    ">  Best for **interactive UIs**, chat applications, and agent reasoning traces\n",
    "\n",
    "---\n",
    "\n",
    "##  **3. `batch()` — Parallel Request Processing**\n",
    "\n",
    "<span style=\"color:#EF6C00;\"><b>Executes multiple independent requests together</b></span>\n",
    "\n",
    "- Processes inputs **in parallel**\n",
    "- Improves **throughput**\n",
    "- Reduces **cost per request**\n",
    "\n",
    "###  Benefits\n",
    "- Efficient for:\n",
    "  - Evaluations\n",
    "  - Bulk inference\n",
    "  - Background processing\n",
    "- Minimizes repeated model overhead\n",
    "\n",
    ">  Best for **high-volume workloads** and **cost-optimized pipelines**\n",
    "\n",
    "---\n",
    "\n",
    "##  **Summary Table**\n",
    "\n",
    "| Method    | Use Case                          | Execution Style | Best For |\n",
    "|----------|-----------------------------------|-----------------|----------|\n",
    "| `invoke` | Single / conversational request   | Synchronous     | Simplicity |\n",
    "| `stream` | Progressive response generation   | Iterative       | UX & interactivity |\n",
    "| `batch`  | Multiple independent requests     | Parallel        | Scale & cost |\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ **Key Takeaway**\n",
    "> <span style=\"color:#C62828;\"><b>Choose the method based on interaction style:</b></span>  \n",
    "> **`invoke` → simplicity**, **`stream` → experience**, **`batch` → scale**\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c05e125-6ac5-4cb1-b3a7-fcd8b578e582",
   "metadata": {},
   "source": [
    "##  **`messages` — Core Input for Model Invocation**\n",
    "\n",
    "<span style=\"color:#6A1B9A;\"><b>`messages`</b></span> is the **primary input key** used when invoking a model in **LangChain**.  \n",
    "It defines the **entire conversational context** passed to the LLM.\n",
    "\n",
    "---\n",
    "\n",
    "##  **What Are Messages?**\n",
    "\n",
    "**Messages** are the **fundamental unit of context** for models in LangChain.  \n",
    "They represent both **inputs and outputs**, carrying:\n",
    "\n",
    "- **Content**\n",
    "- **Role**\n",
    "- **Metadata**\n",
    "\n",
    "Together, these define the **state of the conversation** when interacting with an LLM.\n",
    "\n",
    "---\n",
    "\n",
    "## **Message Structure**\n",
    "\n",
    "Each message typically contains the following components:\n",
    "\n",
    "### **Role**\n",
    "<span style=\"color:#1565C0;\"><b>Identifies the message type</b></span>  \n",
    "Examples:\n",
    "- `system`\n",
    "- `user`\n",
    "- `assistant`\n",
    "- `tool`\n",
    "\n",
    "---\n",
    "\n",
    "### **Content**\n",
    "<span style=\"color:#2E7D32;\"><b>The actual payload of the message</b></span>  \n",
    "May include:\n",
    "- Text\n",
    "- Images\n",
    "- Audio\n",
    "- Documents\n",
    "- Tool instructions\n",
    "\n",
    "---\n",
    "\n",
    "### **Metadata** *(Optional)*\n",
    "<span style=\"color:#EF6C00;\"><b>Additional contextual information</b></span> such as:\n",
    "- Message IDs\n",
    "- Token usage\n",
    "- Response timing\n",
    "- Tool execution details\n",
    "\n",
    "---\n",
    "\n",
    "## **Message Types in LangChain**\n",
    "\n",
    "### **System Message**\n",
    "<span style=\"color:#5E35B1;\"><i>Defines behavior and global context</i></span>  \n",
    "- Sets rules, tone, and constraints\n",
    "- Applied implicitly across the conversation\n",
    "\n",
    "---\n",
    "\n",
    "### **Human Message**\n",
    "<span style=\"color:#1B5E20;\"><i>Represents user input</i></span>  \n",
    "- Questions\n",
    "- Instructions\n",
    "- Prompts\n",
    "\n",
    "---\n",
    "\n",
    "### **AI Message**\n",
    "<span style=\"color:#0D47A1;\"><i>Generated by the model</i></span>  \n",
    "Includes:\n",
    "- Text responses\n",
    "- Tool call requests\n",
    "- Model-generated metadata\n",
    "\n",
    "---\n",
    "\n",
    "### **Tool Message**\n",
    "<span style=\"color:#E65100;\"><i>Output from tool execution</i></span>  \n",
    "- Results returned by tools\n",
    "- Fed back into the model for continued reasoning\n",
    "\n",
    "---\n",
    "\n",
    "## **Key Insight**\n",
    "> <span style=\"color:#C62828;\"><b>`messages` are not just text</b></span> — they are a **structured conversation state** that enables:\n",
    "- Multi-turn reasoning\n",
    "- Tool usage\n",
    "- Memory and context persistence\n",
    "- Agentic workflows\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620a36f5-5444-4a03-9e08-a11d18f7430e",
   "metadata": {},
   "source": [
    "# `init_chat_model` parameters (LangChain)\n",
    "\n",
    "`init_chat_model` creates a provider-agnostic **chat** model from a name and optional configuration.\n",
    "\n",
    "---\n",
    "\n",
    "## Core signature\n",
    "\n",
    "```python\n",
    "from langchain.chat_models import init_chat_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac0cea1-cf8d-4514-95e9-c81c24fd3206",
   "metadata": {},
   "source": [
    "```python\n",
    "init_chat_model(\n",
    "    model: str | None = None,\n",
    "    *,\n",
    "    model_provider: str | None = None,\n",
    "    configurable_fields: Literal[\"any\"] | list[str] | tuple[str, ...] | None = None,\n",
    "    config_prefix: str | None = None,\n",
    "    **kwargs: Any,\n",
    ") -> BaseChatModel | _ConfigurableModel\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d1df6f-511c-46a8-8926-97a7f8af565a",
   "metadata": {},
   "source": [
    "## **Invoking a Chat Model Using `init_chat_model`**\n",
    "\n",
    "This example demonstrates how to initialize and invoke a chat model using LangChain with minimal configuration.\n",
    "\n",
    "---\n",
    "\n",
    "## **Python Example**\n",
    "\n",
    "```python\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "model = init_chat_model(\"openai:gpt-5-nano\")\n",
    "resp = model.invoke(\"Hello, who are you?\")\n",
    "print(resp.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9edd64e-df9e-477b-825f-872689ef2e0c",
   "metadata": {},
   "source": [
    "## **Explicit Chat Model Configuration with `init_chat_model`**\n",
    "\n",
    "This example shows how to **explicitly configure** the model, provider, endpoint, and credentials instead of relying on inference from the model string or environment variables.\n",
    "\n",
    "---\n",
    "\n",
    "## **Python Example**\n",
    "\n",
    "```python\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "model = init_chat_model(\n",
    "    model=\"MODEL_NAME\",\n",
    "    model_provider=\"openai\",\n",
    "    base_url=\"https://your-endpoint.example.com/v1\",\n",
    "    api_key=\"YOUR_API_KEY\",\n",
    ")\n",
    "\n",
    "resp = model.invoke(\"Say hi in one short sentence.\")\n",
    "print(resp.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d96511-d44d-434e-926e-788b5757b547",
   "metadata": {},
   "source": [
    "## **Main Parameters in the `init_chat_model` Configuration Pattern**\n",
    "\n",
    "This pattern allows fine-grained control over **model selection**, **provider routing**, and **runtime behavior** when initializing a chat model.\n",
    "\n",
    "---\n",
    "\n",
    "## **Core Parameters**\n",
    "\n",
    "### **`model`**\n",
    "<span style=\"color:#6A1B9A;\"><b>Logical model identifier</b></span>\n",
    "\n",
    "- Represents the target model name\n",
    "- Examples:\n",
    "  - `gpt-4.1`\n",
    "  - `MODEL_NAME`\n",
    "- Can map to:\n",
    "  - A hosted provider model\n",
    "  - A custom or internally aliased model\n",
    "\n",
    "---\n",
    "\n",
    "### **`model_provider`**\n",
    "<span style=\"color:#1565C0;\"><b>Provider key used for client resolution</b></span>\n",
    "\n",
    "- Determines which provider implementation is used\n",
    "- Common values:\n",
    "  - `openai`\n",
    "  - `anthropic`\n",
    "  - `vertexai`\n",
    "- Required when the provider cannot be inferred automatically\n",
    "\n",
    "---\n",
    "\n",
    "### **`base_url`**\n",
    "<span style=\"color:#2E7D32;\"><b>Optional custom HTTP endpoint</b></span>\n",
    "\n",
    "- Overrides the default provider endpoint\n",
    "- Common use cases:\n",
    "  - Enterprise proxies\n",
    "  - Self-hosted or OpenAI-compatible APIs\n",
    "  - Traffic routing through gateways\n",
    "\n",
    "---\n",
    "\n",
    "### **`api_key`**\n",
    "<span style=\"color:#EF6C00;\"><b>Optional authentication credential</b></span>\n",
    "\n",
    "- Used when environment variables are not preferred\n",
    "- Enables:\n",
    "  - Per-request credential injection\n",
    "  - Multi-tenant or dynamic authentication\n",
    "\n",
    "> In production environments, secret managers or environment variables are recommended.\n",
    "\n",
    "---\n",
    "\n",
    "## **Provider-Specific Arguments (`**kwargs`)**\n",
    "\n",
    "<span style=\"color:#455A64;\"><b>Additional parameters forwarded to the provider client</b></span>\n",
    "\n",
    "- Passed directly to the underlying SDK\n",
    "- Behavior depends on provider support\n",
    "\n",
    "### **Common Examples**\n",
    "- `temperature`\n",
    "- `max_tokens`\n",
    "- `top_p`\n",
    "- `frequency_penalty`\n",
    "\n",
    "These parameters control **generation behavior**, **output length**, and **sampling strategy**.\n",
    "\n",
    "---\n",
    "\n",
    "## **Key Insight**\n",
    "\n",
    "> <span style=\"color:#C62828;\"><b>This pattern cleanly separates concerns</b></span>:\n",
    "- Logical model naming\n",
    "- Provider selection\n",
    "- Endpoint routing\n",
    "- Runtime tuning via `**kwargs`\n",
    "\n",
    "This makes the configuration **portable**, **extensible**, and **enterprise-ready**.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe6a01a-2a9b-45ea-a785-d36bb8df5104",
   "metadata": {},
   "source": [
    "## **Passing Runtime Parameters via `**kwargs` in `init_chat_model`**\n",
    "\n",
    "This example demonstrates how **generation parameters** are passed directly to the underlying provider client using `**kwargs`.\n",
    "\n",
    "---\n",
    "\n",
    "## **Python Example**\n",
    "\n",
    "```python\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "model = init_chat_model(\n",
    "    model=\"openai:gpt-4.1-mini\",\n",
    "    temperature=0.2,\n",
    "    max_tokens=256,\n",
    ")\n",
    "\n",
    "resp = model.invoke(\"Explain RAG in 3 bullet points.\")\n",
    "print(resp.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c11cd21-5a28-4d6d-9e52-35f6d32efe60",
   "metadata": {},
   "source": [
    "## **Runtime Configuration Overrides with `configurable_fields`**\n",
    "\n",
    "This pattern demonstrates how to define **which parameters can be overridden at runtime** when initializing a chat model, without rebuilding the model object.\n",
    "\n",
    "---\n",
    "\n",
    "## **Python Example**\n",
    "\n",
    "```python\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "chat = init_chat_model(\n",
    "    model=\"openai:gpt-4.1-mini\",\n",
    "    temperature=0,\n",
    "    configurable_fields=(\"model\", \"temperature\", \"max_tokens\"),\n",
    "    config_prefix=\"chat\",\n",
    ")\n",
    "\n",
    "# Uses defaults: model=\"openai:gpt-4.1-mini\", temperature=0\n",
    "resp_default = chat.invoke(\"What's your name?\")\n",
    "print(resp_default.content)\n",
    "\n",
    "# Override via config at runtime\n",
    "resp_custom = chat.invoke(\n",
    "    \"What's your name?\",\n",
    "    config={\n",
    "        \"configurable\": {\n",
    "            \"chat_model\": \"openai:gpt-4.1-mini\",\n",
    "            \"chat_temperature\": 0.7,\n",
    "            \"chat_max_tokens\": 128,\n",
    "        }\n",
    "    },\n",
    ")\n",
    "print(resp_custom.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831e30db-1804-4ba9-b97a-aa9c39fefcc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1404b4-99ec-4429-a068-9dc657e82433",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b9bee9-5c93-47ca-abff-8aa962e74f73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3eefc0-9d57-4457-aa1f-fcb532a6ffd3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T07:45:48.742105Z",
     "iopub.status.busy": "2026-01-10T07:45:48.741887Z",
     "iopub.status.idle": "2026-01-10T07:45:48.752312Z",
     "shell.execute_reply": "2026-01-10T07:45:48.751364Z",
     "shell.execute_reply.started": "2026-01-10T07:45:48.742085Z"
    }
   },
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b0aefc-23fd-442c-ac87-abd54179df78",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T07:45:49.234083Z",
     "iopub.status.busy": "2026-01-10T07:45:49.233884Z",
     "iopub.status.idle": "2026-01-10T07:45:50.263099Z",
     "shell.execute_reply": "2026-01-10T07:45:50.262010Z",
     "shell.execute_reply.started": "2026-01-10T07:45:49.234066Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "model = init_chat_model(model=\"gpt-5-nano\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9798a45d-d4e3-4971-ae58-b150b9b84e4e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T07:45:50.263920Z",
     "iopub.status.busy": "2026-01-10T07:45:50.263746Z",
     "iopub.status.idle": "2026-01-10T07:46:04.351765Z",
     "shell.execute_reply": "2026-01-10T07:46:04.350654Z",
     "shell.execute_reply.started": "2026-01-10T07:45:50.263904Z"
    }
   },
   "outputs": [],
   "source": [
    "response = model.invoke(\"What's the problem of Delhi in Winters?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725dcff7-b0ce-4754-9077-d6cc4a4f8cf8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T07:46:04.352356Z",
     "iopub.status.busy": "2026-01-10T07:46:04.352188Z",
     "iopub.status.idle": "2026-01-10T07:46:04.356792Z",
     "shell.execute_reply": "2026-01-10T07:46:04.355879Z",
     "shell.execute_reply.started": "2026-01-10T07:46:04.352340Z"
    }
   },
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7dc079-e3ce-40d6-9d3f-01fa9157ccfa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T07:46:04.357395Z",
     "iopub.status.busy": "2026-01-10T07:46:04.357235Z",
     "iopub.status.idle": "2026-01-10T07:46:04.361332Z",
     "shell.execute_reply": "2026-01-10T07:46:04.360437Z",
     "shell.execute_reply.started": "2026-01-10T07:46:04.357377Z"
    }
   },
   "outputs": [],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368481d2-0cce-4bc3-8be5-0f5baaf3ab66",
   "metadata": {},
   "source": [
    "```Type: AIMessage object (not a dict or list)\n",
    "\n",
    "Key attributes:\n",
    "- content (str): The main response text\n",
    "- additional_kwargs (dict): Extra metadata like refusal\n",
    "- response_metadata (dict): Contains:\n",
    "  - token_usage: Token counts and details\n",
    "  - model_provider: \"openai\" \n",
    "  - model_name: \"gpt-5-nano-2025-08-07\"\n",
    "  - system_fingerprint, id, service_tier, finish_reason, logprobs\n",
    "- id (str): Unique run identifier\n",
    "- tool_calls (list): Empty list\n",
    "- invalid_tool_calls (list): Empty list  \n",
    "- usage_metadata (dict): Token usage summary\n",
    "\n",
    "Access examples:\n",
    "python\n",
    "x.content  # Get the text response\n",
    "x.response_metadata['token_usage']['total_tokens']  # Get token count\n",
    "x.response_metadata['model_name']  # Get model used\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d2d406-538d-487a-b1f3-c0377d3cc12e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T07:46:04.362123Z",
     "iopub.status.busy": "2026-01-10T07:46:04.361971Z",
     "iopub.status.idle": "2026-01-10T07:46:04.366531Z",
     "shell.execute_reply": "2026-01-10T07:46:04.365690Z",
     "shell.execute_reply.started": "2026-01-10T07:46:04.362108Z"
    }
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "pprint(response.response_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa54c203-9a13-4069-b60f-6cfd2f8ef9df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T07:46:04.367596Z",
     "iopub.status.busy": "2026-01-10T07:46:04.367441Z",
     "iopub.status.idle": "2026-01-10T07:46:04.372101Z",
     "shell.execute_reply": "2026-01-10T07:46:04.371026Z",
     "shell.execute_reply.started": "2026-01-10T07:46:04.367581Z"
    }
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "pprint(response.response_metadata['token_usage'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8350d64-0ffa-4fbd-b843-2a2af9520fbe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T07:46:04.372742Z",
     "iopub.status.busy": "2026-01-10T07:46:04.372572Z",
     "iopub.status.idle": "2026-01-10T07:46:04.377959Z",
     "shell.execute_reply": "2026-01-10T07:46:04.377066Z",
     "shell.execute_reply.started": "2026-01-10T07:46:04.372728Z"
    }
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "pprint(response.response_metadata['token_usage']['total_tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42810aea-198a-41a7-a498-ee4d8e4841ba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T07:46:04.378951Z",
     "iopub.status.busy": "2026-01-10T07:46:04.378791Z",
     "iopub.status.idle": "2026-01-10T07:46:04.384177Z",
     "shell.execute_reply": "2026-01-10T07:46:04.383303Z",
     "shell.execute_reply.started": "2026-01-10T07:46:04.378937Z"
    }
   },
   "outputs": [],
   "source": [
    "## Shows all attributes/methods\n",
    "dir(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d55387e-2f72-48b7-ae46-829c9c1b13f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T07:46:04.384939Z",
     "iopub.status.busy": "2026-01-10T07:46:04.384720Z",
     "iopub.status.idle": "2026-01-10T07:46:04.389361Z",
     "shell.execute_reply": "2026-01-10T07:46:04.388354Z",
     "shell.execute_reply.started": "2026-01-10T07:46:04.384918Z"
    }
   },
   "outputs": [],
   "source": [
    "# Show Dictonary Keys\n",
    "response.response_metadata.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32db93f-d3e1-4223-b69c-700f7f50daaa",
   "metadata": {},
   "source": [
    "## Custominzing the model using prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d8d1d9-678c-437e-938d-e4d89dd68497",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T07:46:04.389988Z",
     "iopub.status.busy": "2026-01-10T07:46:04.389833Z",
     "iopub.status.idle": "2026-01-10T07:46:12.009880Z",
     "shell.execute_reply": "2026-01-10T07:46:12.008890Z",
     "shell.execute_reply.started": "2026-01-10T07:46:04.389973Z"
    }
   },
   "outputs": [],
   "source": [
    "model = init_chat_model(\n",
    "    model = 'gpt-5-nano',\n",
    "    temperature=1.0\n",
    ")\n",
    "\n",
    "response = model.invoke(\"What's the Capital of Milky Way?\")\n",
    "pprint(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43505802-54df-4777-992a-f3662de3ec4f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T07:46:12.010580Z",
     "iopub.status.busy": "2026-01-10T07:46:12.010382Z",
     "iopub.status.idle": "2026-01-10T07:46:12.013994Z",
     "shell.execute_reply": "2026-01-10T07:46:12.013091Z",
     "shell.execute_reply.started": "2026-01-10T07:46:12.010564Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a151a40d-04e3-4d65-9533-794c78852540",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T07:46:12.014634Z",
     "iopub.status.busy": "2026-01-10T07:46:12.014477Z",
     "iopub.status.idle": "2026-01-10T07:46:17.532203Z",
     "shell.execute_reply": "2026-01-10T07:46:17.531242Z",
     "shell.execute_reply.started": "2026-01-10T07:46:12.014619Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "model = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash-lite\")\n",
    "\n",
    "response = model.invoke(\"Complete this YoYo Honey Singh, jaisai aaloo kai pakodai\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c342ec48-21f8-4b01-b7cc-4cd244bd59c1",
   "metadata": {},
   "source": [
    "## Initialising and invoking an agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab4ab58-9407-4eed-b0e4-1201e9499129",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T07:46:17.533043Z",
     "iopub.status.busy": "2026-01-10T07:46:17.532738Z",
     "iopub.status.idle": "2026-01-10T07:46:17.597633Z",
     "shell.execute_reply": "2026-01-10T07:46:17.596473Z",
     "shell.execute_reply.started": "2026-01-10T07:46:17.533026Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "model = \"gpt-5-nano\"\n",
    "agent = create_agent(model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff54dfda-857d-48f8-a8f0-8ada33cdb11b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T07:46:17.601585Z",
     "iopub.status.busy": "2026-01-10T07:46:17.601410Z",
     "iopub.status.idle": "2026-01-10T07:46:29.922034Z",
     "shell.execute_reply": "2026-01-10T07:46:29.921153Z",
     "shell.execute_reply.started": "2026-01-10T07:46:17.601568Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.messages import HumanMessage\n",
    "response = agent.invoke(\n",
    "    { \"messages\": [HumanMessage (content = \"What is Capital of Universe?\") ] }\n",
    ")\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bc156c-de2d-48d3-8a77-87004e65c6b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T07:46:29.923305Z",
     "iopub.status.busy": "2026-01-10T07:46:29.922982Z",
     "iopub.status.idle": "2026-01-10T07:46:29.927390Z",
     "shell.execute_reply": "2026-01-10T07:46:29.926414Z",
     "shell.execute_reply.started": "2026-01-10T07:46:29.923280Z"
    }
   },
   "outputs": [],
   "source": [
    "print(response['messages'][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5c7393-8268-4b71-a33f-3a1278af7f72",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T07:46:29.930663Z",
     "iopub.status.busy": "2026-01-10T07:46:29.930494Z",
     "iopub.status.idle": "2026-01-10T07:46:42.981780Z",
     "shell.execute_reply": "2026-01-10T07:46:42.980720Z",
     "shell.execute_reply.started": "2026-01-10T07:46:29.930648Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.messages import AIMessage\n",
    "\n",
    "response = agent.invoke(\n",
    "    {\"messages\" : [HumanMessage(content = \"What is Capital of Universe?\"), \n",
    "                   AIMessage(content=\"The Capital of the Universe if DoreMon\"),\n",
    "                   HumanMessage(content=\"Interesting, tell me capital of Milkyway\")\n",
    "                  ]}\n",
    ")\n",
    "\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584e334e-48fe-471b-8338-c3f5a9a0f04a",
   "metadata": {},
   "source": [
    "## Streaming Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07633740-98b3-4eb8-80b8-3c33c47ae2ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T07:46:42.982391Z",
     "iopub.status.busy": "2026-01-10T07:46:42.982230Z",
     "iopub.status.idle": "2026-01-10T07:47:02.988372Z",
     "shell.execute_reply": "2026-01-10T07:47:02.987468Z",
     "shell.execute_reply.started": "2026-01-10T07:46:42.982375Z"
    }
   },
   "outputs": [],
   "source": [
    "for token, metadata in agent.stream(\n",
    "    {\"messages\" : [HumanMessage(content=\"Tell me about Galactic Council\")]},\n",
    "    stream_mode=\"messages\"\n",
    "):\n",
    "    if token.content:\n",
    "        print(token.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d1fc6c-dc52-454d-a51b-2160f23b3b82",
   "metadata": {},
   "source": [
    "#### Streaming Model Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0176aae8-68cd-4b1a-abcf-957ff396a955",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T07:50:57.802542Z",
     "iopub.status.busy": "2026-01-10T07:50:57.802358Z",
     "iopub.status.idle": "2026-01-10T07:51:05.209221Z",
     "shell.execute_reply": "2026-01-10T07:51:05.208144Z",
     "shell.execute_reply.started": "2026-01-10T07:50:57.802525Z"
    }
   },
   "outputs": [],
   "source": [
    "model = init_chat_model(\n",
    "    model = 'gpt-5-nano',\n",
    "    temperature=1.0\n",
    ")\n",
    "\n",
    "response = model.invoke(\"What's the Capital of Milky Way?\")\n",
    "pprint(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19844177-3fc1-42f3-b7b3-5a8ecd01d186",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T07:53:15.032608Z",
     "iopub.status.busy": "2026-01-10T07:53:15.032380Z",
     "iopub.status.idle": "2026-01-10T07:53:31.701622Z",
     "shell.execute_reply": "2026-01-10T07:53:31.700562Z",
     "shell.execute_reply.started": "2026-01-10T07:53:15.032592Z"
    }
   },
   "outputs": [],
   "source": [
    "for chunk in model.stream(\"Why this Kolaveri di aa di?\"):\n",
    "    print(chunk.text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae48d684-9d5f-427b-acdb-1f6d83359487",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T07:54:04.184968Z",
     "iopub.status.busy": "2026-01-10T07:54:04.184748Z",
     "iopub.status.idle": "2026-01-10T07:54:08.736129Z",
     "shell.execute_reply": "2026-01-10T07:54:08.735088Z",
     "shell.execute_reply.started": "2026-01-10T07:54:04.184951Z"
    }
   },
   "outputs": [],
   "source": [
    "full = None  # None | AIMessageChunk\n",
    "for chunk in model.stream(\"What color is the sky?\"):\n",
    "    full = chunk if full is None else full + chunk\n",
    "    print(full.text)\n",
    "\n",
    "# The\n",
    "# The sky\n",
    "# The sky is\n",
    "# The sky is typically\n",
    "# The sky is typically blue\n",
    "# ...\n",
    "\n",
    "print(full.content_blocks)\n",
    "# [{\"type\": \"text\", \"text\": \"The sky is typically blue...\"}]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d2cb55-80d0-4653-b078-18280dc323e2",
   "metadata": {},
   "source": [
    "### Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f635b3f-175b-4196-839d-a2abdddd5f2e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T08:02:10.857760Z",
     "iopub.status.busy": "2026-01-10T08:02:10.857545Z",
     "iopub.status.idle": "2026-01-10T08:02:25.393725Z",
     "shell.execute_reply": "2026-01-10T08:02:25.392910Z",
     "shell.execute_reply.started": "2026-01-10T08:02:10.857742Z"
    }
   },
   "outputs": [],
   "source": [
    "responses = model.batch([\n",
    "    \"Why do parrots have colorful feathers?\",\n",
    "    \"How do airplanes fly?\",\n",
    "    \"What is quantum computing?\"\n",
    "])\n",
    "for response in responses:\n",
    "    pprint(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc4a1ef-5908-427f-ac4a-6ef26e6fe89d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T08:03:34.048790Z",
     "iopub.status.busy": "2026-01-10T08:03:34.048549Z",
     "iopub.status.idle": "2026-01-10T08:03:47.540179Z",
     "shell.execute_reply": "2026-01-10T08:03:47.539205Z",
     "shell.execute_reply.started": "2026-01-10T08:03:34.048773Z"
    }
   },
   "outputs": [],
   "source": [
    "for response in model.batch_as_completed([\n",
    "    \"Why do parrots have colorful feathers?\",\n",
    "    \"How do airplanes fly?\",\n",
    "    \"What is quantum computing?\"\n",
    "]):\n",
    "    pprint(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d508a2c5-825e-454c-9bfd-8c295f832564",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lca-foundations",
   "language": "python",
   "name": "lca-foundations"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
