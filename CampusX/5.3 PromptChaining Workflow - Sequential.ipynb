{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c92c082a-aa40-44cd-b715-b219562beef5",
   "metadata": {},
   "source": [
    "# PromptChaining Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7897ce9f-8232-4089-86d0-6fe00bf7e7bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T11:35:30.389631Z",
     "iopub.status.busy": "2026-01-16T11:35:30.389273Z",
     "iopub.status.idle": "2026-01-16T11:35:37.592900Z",
     "shell.execute_reply": "2026-01-16T11:35:37.581164Z",
     "shell.execute_reply.started": "2026-01-16T11:35:30.389614Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install langextract - Ignore it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14de5a2d-aa0f-4f85-8d5f-c627d14e52e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T11:24:51.246306Z",
     "iopub.status.busy": "2026-01-16T11:24:51.245794Z",
     "iopub.status.idle": "2026-01-16T11:24:51.255819Z",
     "shell.execute_reply": "2026-01-16T11:24:51.255267Z",
     "shell.execute_reply.started": "2026-01-16T11:24:51.246286Z"
    }
   },
   "outputs": [],
   "source": [
    "# Importing necessary libraries \n",
    "\n",
    "from langgraph.graph import StateGraph, START, END # To created Graph\n",
    "from langchain.chat_models import init_chat_model # To interact with Models\n",
    "from typing import TypedDict, List # To create Struct for Graph State\n",
    "from dotenv import load_dotenv # To load the env variables\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de64d2ce-f640-47da-af10-7314540403a3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T11:54:24.871525Z",
     "iopub.status.busy": "2026-01-16T11:54:24.868880Z",
     "iopub.status.idle": "2026-01-16T11:54:24.882019Z",
     "shell.execute_reply": "2026-01-16T11:54:24.881445Z",
     "shell.execute_reply.started": "2026-01-16T11:54:24.871504Z"
    }
   },
   "outputs": [],
   "source": [
    "# Creating a Struct using TypedDict to store the state of graph\n",
    "\n",
    "class BlogState(TypedDict):\n",
    "\n",
    "    topic : \"str\"\n",
    "    outline : \"str\"\n",
    "    blog : \"str\"\n",
    "    tags : list[str] = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135da454-e9e4-427a-a98d-aa8b0800160e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T11:54:24.988807Z",
     "iopub.status.busy": "2026-01-16T11:54:24.983808Z",
     "iopub.status.idle": "2026-01-16T11:54:26.819011Z",
     "shell.execute_reply": "2026-01-16T11:54:26.815984Z",
     "shell.execute_reply.started": "2026-01-16T11:54:24.988788Z"
    }
   },
   "outputs": [],
   "source": [
    "# Creating just to expirement will use this to pass as a parameter to the get_blog_func function \n",
    "\n",
    "# Create an object of init_chat_model class\n",
    "model = init_chat_model(\"gpt-5-nano\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ffdb7b-c714-4ee8-908c-51e67f333f80",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T11:54:26.821623Z",
     "iopub.status.busy": "2026-01-16T11:54:26.820542Z",
     "iopub.status.idle": "2026-01-16T11:54:26.830774Z",
     "shell.execute_reply": "2026-01-16T11:54:26.830155Z",
     "shell.execute_reply.started": "2026-01-16T11:54:26.821606Z"
    }
   },
   "outputs": [],
   "source": [
    "# Defining the get_outline_func for the node 1 \n",
    "\n",
    "def get_outline_func(state : BlogState) -> BlogState:\n",
    "    \n",
    "    # I am extracting the topic that the user will provide as input and it will get stored in struct - BlogState\n",
    "    topic = state[\"topic\"]\n",
    "\n",
    "    # Once I have extracted the topic - I will draft a prompt here to pass it to model ( Note : I can take prompt as an input too - It can provide flexibility but also can cause issue in quality of output\n",
    "    prompt = f\"Create an outline for this topic = {topic}, Do not use markdown, headings, or extra formatting. - Just output the outline\"\n",
    "\n",
    "    # Now I can pass the prompt to model - but how I can call model here inside the function\n",
    "    # that's the reason we imported langchain.chat_model \n",
    "    # Create an object of init_chat_model class\n",
    "    model = init_chat_model(\"gpt-5-nano\")\n",
    "\n",
    "    # I can use the the instance \"model\" to call function inside the init_chat_model()\n",
    "    response = model.invoke(prompt)\n",
    "\n",
    "    # \"response\" will give a raw response form the invoke model along with other attributes, I just want the response to my prompt not other metadata as of now\n",
    "    answer = response.content\n",
    "\n",
    "    # saving the answer to state of graph - updating the state\n",
    "    state[\"outline\"] = answer\n",
    "\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5f721b-b2a8-4c4c-8ec8-9e862cd6a9cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T11:54:26.831537Z",
     "iopub.status.busy": "2026-01-16T11:54:26.831364Z",
     "iopub.status.idle": "2026-01-16T11:54:26.849974Z",
     "shell.execute_reply": "2026-01-16T11:54:26.849511Z",
     "shell.execute_reply.started": "2026-01-16T11:54:26.831522Z"
    }
   },
   "outputs": [],
   "source": [
    "# Defining the \"get_blog_func\" for the node 2\n",
    "\n",
    "def get_blog_func(state : BlogState, model = model) -> BlogState:\n",
    "\n",
    "    # If we are calling this function, we already must have topic and outline \n",
    "    # getting the topic and outline to pass into the next prompt to create the blog \n",
    "    topic = state[\"topic\"]\n",
    "    outline = state[\"outline\"]\n",
    "\n",
    "    # now we got the parameters , let's create the prompt\n",
    "    prompt = f\"Create a detailed and factually  for this topic = {topic} for this outline = {outline}, Do not use markdown, headings, emoji, icons or extra formatting. - Just output the blog - Ready to post\"\n",
    "\n",
    "    # now passing this prompt to model\n",
    "    response = model.invoke(prompt)\n",
    "\n",
    "    # I can use the the instance \"model\" to call function inside the init_chat_model()\n",
    "    response = model.invoke(prompt)\n",
    "\n",
    "    # \"response\" will give a raw response form the invoke model along with other attributes, I just want the response to my prompt not other metadata as of now\n",
    "    answer = response.content\n",
    "\n",
    "    # saving the answer to state of graph - updating the state\n",
    "    state[\"blog\"] = answer\n",
    "\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48cc5001-eef8-4334-9a1c-8ed1ada14bb4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T12:04:51.448772Z",
     "iopub.status.busy": "2026-01-16T12:04:51.446946Z",
     "iopub.status.idle": "2026-01-16T12:04:51.457890Z",
     "shell.execute_reply": "2026-01-16T12:04:51.457318Z",
     "shell.execute_reply.started": "2026-01-16T12:04:51.448750Z"
    }
   },
   "outputs": [],
   "source": [
    "# Defining the graph \n",
    "\n",
    "# Let's see what's happening here\n",
    "\"\"\"\n",
    "1. Using \"StateGraph\" prebuilt class to create an instance of the same class \n",
    "2. Passing \"BlogState\" as a Argument\n",
    "\"\"\"\n",
    "blog_graph = StateGraph(BlogState)\n",
    "\n",
    "# Adding Nodes\n",
    "blog_graph.add_node(\"get_outline_node\", get_outline_func)\n",
    "blog_graph.add_node(\"get_blog_node\", get_blog_func)\n",
    "\n",
    "# Adding Edges\n",
    "blog_graph.add_edge(START, \"get_outline_node\")\n",
    "blog_graph.add_edge(\"get_outline_node\", \"get_blog_node\")\n",
    "blog_graph.add_edge(\"get_blog_node\", END)\n",
    "\n",
    "# Compiling the Graph\n",
    "workflow = blog_graph.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888f8ea0-902b-40b1-904e-9db36bb92398",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T12:04:55.366766Z",
     "iopub.status.busy": "2026-01-16T12:04:55.366440Z",
     "iopub.status.idle": "2026-01-16T12:04:58.341899Z",
     "shell.execute_reply": "2026-01-16T12:04:58.340456Z",
     "shell.execute_reply.started": "2026-01-16T12:04:55.366749Z"
    }
   },
   "outputs": [],
   "source": [
    "workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0bf016b-cbcc-4d6d-8704-2635ba09ea12",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_state = \"Quantum Computing & Generative AI\"\n",
    "\n",
    "final_state = "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
